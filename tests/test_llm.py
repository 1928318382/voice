#!/usr/bin/env python3
"""
æµ‹è¯•é˜¿é‡Œäº‘ç™¾ç‚¼ LLM API æ˜¯å¦èƒ½æ­£å¸¸è°ƒç”¨
"""

import os
import sys
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.config import LLM_API_KEY, LLM_BASE_URL, LLM_MODEL_NAME

def test_llm_api():
    """æµ‹è¯• LLM API è¿æ¥å’Œè°ƒç”¨"""
    print("æ­£åœ¨æµ‹è¯• LLM API...")
    print(f"API Key: {LLM_API_KEY[:8]}...{LLM_API_KEY[-4:] if len(LLM_API_KEY) > 12 else LLM_API_KEY}")
    print(f"Base URL: {LLM_BASE_URL}")
    print(f"Model: {LLM_MODEL_NAME}")
    print("-" * 50)

    try:
        from openai import OpenAI

        client = OpenAI(
            api_key=LLM_API_KEY,
            base_url=LLM_BASE_URL
        )

        print("æ­£åœ¨å‘é€æµ‹è¯•è¯·æ±‚...")

        response = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç®€æ´å‹å¥½åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"},
                {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
            ],
            temperature=0.7,
            max_tokens=200
        )

        content = response.choices[0].message.content
        print("âœ… API è°ƒç”¨æˆåŠŸï¼")
        print(f"æ¨¡å‹å›å¤: {content}")
        return True

    except Exception as e:
        print(f"âŒ API è°ƒç”¨å¤±è´¥: {e}")
        print("\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®")
        print("2. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("3. æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
        print("4. æ£€æŸ¥è´¦æˆ·æ˜¯å¦æœ‰ä½™é¢")
        print("5. æ£€æŸ¥ base_url æ˜¯å¦æ­£ç¡®")
        return False

if __name__ == "__main__":
    success = test_llm_api()
    if success:
        print("\nğŸ‰ å¤§æ¨¡å‹APIé…ç½®æ­£ç¡®ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼")
    else:
        print("\nâŒ è¯·æ£€æŸ¥é…ç½®åé‡è¯•")
        sys.exit(1)